---
title: "Impact of Subgroup Misclassification on Detecting Heterogeneous Treatment Effects in *Staphylococcus aureus* Bacteremia: A Simulation Study"
author: "Fergus Hamilton"
date: "`r format(Sys.Date(), '%B %d, %Y')`" # Add date
format:
  html:
    self-contained: true
    toc: true
    code-fold: true
    code-summary: "Show code"
    number-sections: true # Add section numbering
editor: visual
execute:
  echo: true        # Show code chunks
  warning: false    # Hide warnings
  message: false    # Hide messages
  error: true       # Show errors
bibliography: references.bib # Specify bibliography file if needed
csl: vancouver.csl         # Specify citation style if needed
---

```{r setup, include=FALSE}
# Load necessary packages
pacman::p_load(
  tidyverse,
  broom,
  ggdist,
  gt,
  patchwork,
  scales, # For formatting numbers/percentages
  future, # For parallel processing
  furrr   # For parallel map functions
)

# Set theme for plots
theme_set(theme_minimal() + theme(legend.position = "bottom"))

# --- Helper Functions Defined Internally ---
# Define these *before* sourcing, just in case the sourced file relies on them
# or to ensure these versions are definitely used.

# Function to misclassify group (Based on logic within user's estimate_effect_misclassify)
misclassify_group <- function(sim_data, accuracy, freq_vector, seed = NULL) {
  if(!is.null(seed)) set.seed(seed)
  n <- nrow(sim_data)
  group_labels <- names(freq_vector)
  if(is.null(group_labels)) group_labels <- LETTERS[1:length(freq_vector)]
  names(freq_vector) <- group_labels # Ensure names are set
  freq_normalized <- freq_vector / sum(freq_vector)

  # Ensure input data has 'group' column
  if (!"group" %in% names(sim_data)) stop("Input data must have a 'group' column.")

  sim_data_with_assigned <- sim_data |>
    dplyr::mutate(
      is_correct = rbinom(dplyr::n(), 1, accuracy),
      assigned_group = dplyr::case_when(
        is_correct == 1 ~ group,
        # Ensure sampling uses the normalized, named vector
        TRUE ~ sample(names(freq_normalized), dplyr::n(), replace = TRUE, prob = freq_normalized)
      )
    ) %>%
    select(-is_correct) # Remove intermediate column
  return(sim_data_with_assigned)
}


# Helper function to fit GLM and extract results safely
# Handles cases with insufficient data or model convergence errors
fit_glm_safe <- function(data) {
  # Check for sufficient data and variation before attempting GLM
  if (nrow(data) < 10 || length(unique(data$treatment)) < 2 || length(unique(data$success)) < 2) {
    return(tibble(beta = NA_real_, se = NA_real_, pval = NA_real_))
  }
  tryCatch({
    # Ensure treatment is factor with levels 0, 1 for glm reference level
    data_glm <- data %>% mutate(treatment = factor(treatment, levels = c(0, 1)))
    model <- glm(success ~ treatment, data = data_glm, family = binomial())
    tidy_model <- broom::tidy(model)
    # Find the treatment effect row (usually treatment1 when factor)
    treatment_row <- tidy_model %>% filter(str_detect(term, "^treatment"))

    if (nrow(treatment_row) == 1) {
      tibble(
        beta = treatment_row$estimate,
        se = treatment_row$std.error,
        pval = treatment_row$p.value
      )
    } else {
      # Handle cases where treatment effect term isn't found (e.g., perfect separation)
      tibble(beta = NA_real_, se = NA_real_, pval = NA_real_)
    }
  }, error = function(e) {
    # Handle any other GLM errors
    # message("GLM error: ", e$message) # Optional: print error message
    tibble(beta = NA_real_, se = NA_real_, pval = NA_real_)
  })
}

# Source the simulation functions using a relative path
# Assumes simulate_trial_data and replicate_sims are defined here
# Note: replicate_sims likely calls estimate_effect_misclassify internally
source("../code/functions/functions.R") # Assuming this file exists relative to the QMD

# Set seed for overall reproducibility
set.seed(20240423)
```



# Abstract

**Background:** *Staphylococcus aureus* bacteremia (SAB) exhibits significant clinical heterogeneity. Recently identified subphenotypes show potential for heterogeneous treatment effects (HTE), but the impact of inevitable patient misclassification on detecting HTE is unclear.

**Methods:** We conducted a simulation study following the ADEMP framework (Aims, Data-generating mechanisms, Estimands, Methods, Performance measures). We simulated clinical trial data based on parameters from the ARREST trial (adjunctive rifampicin vs. placebo for SAB), incorporating five subphenotypes with differential treatment effects on 84-day mortality (Odds Ratios \[ORs\] from 0.3 to 18.8). We assessed: (1) statistical power for post-hoc subgroup analysis versus total trial size assuming perfect classification; (2) the impact of varying classification accuracy (50%-100%) on power, bias, MSE, and wrong-direction estimate rate in post-hoc analyses (fixed N=10,000); and (3) the Number Needed to Screen (NNS) and Number Needed to Randomize (NNR) to achieve 80% power in enrichment trials targeting specific subgroups, considering test accuracy. Simulations were repeated using more moderate ('realistic') ORs.

**Results:** Aim 1 showed substantial total trial sizes (\>20,000) are needed for adequate power (80%) in post-hoc analyses of some subgroups (B, C, E) even with perfect classification, largely driven by low baseline event rates or moderate effect sizes combined with multiple testing correction. Aim 2 demonstrated that decreasing classification accuracy markedly reduced power and increased bias (towards the null) and the risk of estimating effects in the wrong direction in post-hoc analyses. Aim 3 showed that enrichment designs require large NNS, increasing dramatically as test accuracy decreases; for subgroup B (OR=18.8), NNS exceeded 50,000 even with 95% accuracy. Results were less extreme but directionally similar for realistic ORs.

**Conclusions:** Detecting HTE in SAB is challenging. Post-hoc analyses require very large trials and high classification accuracy. Enrichment strategies can reduce NNR but face substantial screening burdens (NNS) heavily influenced by test accuracy and subgroup prevalence. Robust classification methods are crucial for advancing stratified medicine approaches in SAB.

# Introduction

*Staphylococcus aureus* bacteremia (SAB) is a common and serious infection associated with significant morbidity and mortality [@Tong2015]. Globally, *S. aureus* is a leading cause of death due to bacterial pathogens and bacteremia [@GBD2019]. A defining feature of SAB is its clinical heterogeneity, encompassing variations in patient characteristics (e.g., age, comorbidities), pathogen factors (e.g., methicillin resistance), source of infection, and disease severity [@Tong2015]. Despite this heterogeneity, clinical trials in SAB often treat it as a single entity, potentially obscuring differential treatment effects within patient subgroups [@Thwaites2018; Holland2022]. Strategy trials investigating adjunctive or alternative therapies have frequently failed to show overall benefit compared to standard care [@Thwaites2018; Paulsen2024].

Recent efforts have focused on identifying clinically relevant subphenotypes within SAB to enable better patient stratification for research and potentially personalized treatment [@Davis2023]. Swets, Russell, et al. recently used latent class analysis on data from observational and trial cohorts (Edinburgh, ARREST, SAFO) to identify five distinct and reproducible clinical subphenotypes (A-E) based on routinely collected clinical data [@Swets2024]. Crucially, a secondary analysis of the ARREST trial (Adjunctive Rifampicin for *S. aureus* Bacteraemia) suggested potential heterogeneous treatment effects (HTE) of adjunctive rifampicin across these subphenotypes regarding 84-day mortality. Notably, rifampicin appeared potentially harmful in subphenotype B (Nosocomial IV catheter-associated SAB; OR 18.8) and potentially beneficial in subphenotype E (SAB associated with injecting drug use; OR 0.3) [@Swets2024].

The possibility of such HTE raises critical questions for future clinical trial design and the implementation of stratified medicine approaches. If treatment effects truly differ between subgroups, accurately identifying these subgroups becomes paramount. However, any diagnostic test or classification algorithm used to assign patients to subphenotypes will inevitably have imperfect accuracy [@Siontis2014]. Misclassifying patients can lead to biased estimates of subgroup-specific effects, reduced statistical power to detect true HTE, and potentially misleading conclusions about which patients benefit or are harmed by a treatment [@Kent2018; Sussman2017]. Understanding the quantitative impact of misclassification is essential for interpreting subgroup analyses and designing efficient trials, including potential enrichment strategies [@Anthenelli2011; Simon2004].

This simulation study aims to quantify the impact of subgroup misclassification on detecting HTE in SAB, using the subphenotypes and treatment effects derived from the Swets et al. analysis of the ARREST trial as a motivating example. Specifically, we address three aims: 1. Estimate the total sample sizes required in a standard randomized controlled trial (RCT) to achieve adequate statistical power (80%) for post-hoc analyses of subgroup-specific treatment effects, assuming perfect patient classification. 2. Quantify the impact of varying levels of classification accuracy on statistical power, estimation bias, Mean Squared Error (MSE), and the probability of estimating effects in the wrong direction for both subgroup-specific and overall treatment effects in post-hoc analyses of a large (N=10,000) standard RCT. 3. Estimate the Number Needed to Screen (NNS) and the resulting average Number Needed to Randomize (NNR) within the enriched cohort required to achieve 80% power in a hypothetical enrichment trial design targeting specific subgroups, considering varying levels of test accuracy.

We structure the reporting of our simulation methods and results following the ADEMP framework (Aims, Data-generating mechanisms, Estimands, Methods, Performance measures) [@Morris2019].

# Methods

This simulation study was designed and reported following the ADEMP framework [@Morris2019; Siepe2024].

## Aims

1.  **Post-hoc Power vs. N:** To estimate the total sample size (`n`) required in a standard two-arm RCT to achieve 80% statistical power for detecting subgroup-specific treatment effects (based on ARREST trial ORs) in post-hoc analyses, assuming perfect classification (`accuracy`=1.0).
2.  **Impact of Accuracy:** To quantify the impact of varying classification `accuracy` (0.5 to 1.0) on statistical power, bias, MSE, and the wrong-direction rate for estimating subgroup-specific and overall treatment effects in post-hoc analyses of a standard RCT with a fixed total sample size (`n`=10,000).
3.  **Enrichment Trial Sample Sizes:** To estimate the Number Needed to Screen (NNS) and the average Number Needed to Randomize (NNR) within an enriched cohort required to achieve 80% power in a hypothetical enrichment trial targeting specific subgroups (B, C, E), using a screening test with varying `accuracy`.

## Data-Generating Mechanisms (DGMs)

We simulated individual patient data for two-arm (control vs. treatment) RCTs. The core DGM involved the following steps for each simulated patient:

1.  **True Subgroup Assignment:** Each patient was assigned a 'true' subgroup (A, B, C, D, or E) based on sampling from a multinomial distribution defined by the population prevalence (`freq_vector`).
2.  **Treatment Assignment:** Patients were assigned to treatment (1) or control (0) with equal probability (0.5).
3.  **Outcome Generation:** A binary outcome ('success', representing death by 84 days = 1, survival = 0) was generated based on the patient's true subgroup, treatment assignment, the subgroup-specific baseline event probability in the control group (`p0_vector`), and the subgroup-specific treatment effect (odds ratio, `or_vector`). The probability of death for patient `i` in subgroup `j` receiving treatment `t` (0 or 1) was calculated using the logistic model: $P(\text{Death}_{ij} | \text{Treatment}=t) = \text{expit}(\text{logit}(p0_j) + \log(OR_j) \times t)$. The outcome was then drawn from a Bernoulli distribution with this probability.

**Parameterization:** Two main parameter sets were used: \* **ARREST Scenario:** Based directly on the Swets et al. [@Swets2024] analysis of the ARREST trial 84-day mortality data. \* `or_vector`: `c(A = 1.0, B = 18.8, C = 0.79, D = 1.4, E = 0.3)` \* `freq_vector`: `c(A = 60/388, B = 52/388, C = 138/388, D = 69/388, E = 69/388)` \* `p0_vector_adjusted`: `c(A = 7/60, B = 0.5/(52+0.5), C = 11/138, D = 11/69, E = 1/69)`. Note the Haldane-Anscombe correction for subgroup B. \* **Realistic Scenario:** Used more moderate ORs while keeping frequencies and baseline risks the same as the ARREST scenario. \* `or_vector`: `c(A = 1.0, B = 2.0, C = 0.7, D = 1.2, E = 0.8)` \* `freq_vector`: Same as ARREST. \* `p0_vector_adjusted`: Same as ARREST.

**Misclassification / Testing Mechanism:** \* For Aims 2 and 3, patient classification accuracy was simulated using the `misclassify_group` function. For each patient with true subgroup `j`, the assigned subgroup was set to `j` with probability `accuracy`. With probability `1 - accuracy`, the assigned subgroup was randomly sampled from the overall population distribution (`freq_vector`). This simulates a classification process where errors result in assignment proportional to overall prevalence.

## Estimands

The target quantities (estimands) for each aim were:

-   **Aim 1:** The statistical power to reject the null hypothesis of no treatment effect (OR=1) within each subgroup (A-E) in a post-hoc analysis, using a Bonferroni-corrected alpha level (0.05 / 5 = 0.01).
-   **Aim 2:**
    -   Primary: Statistical power (as in Aim 1, plus overall power at alpha=0.05).
    -   Secondary: Bias (mean difference between estimated log(OR) and true log(OR)), Mean Squared Error (MSE) of the log(OR) estimate, and Wrong Direction Rate (percentage of estimates where sign(log(OR)) differs from sign(true log(OR))).
-   **Aim 3:**
    -   Primary: Number Needed to Screen (NNS) to achieve 80% power (alpha=0.05) in an enrichment trial for a target subgroup.
    -   Secondary: Average Number Needed to Randomize (NNR) within the enriched cohort corresponding to the NNS achieving 80% power.

## Methods (Simulation and Analysis)

-   **Simulation Structure:**
    -   **Aim 1:** Iterated through `sample_sizes`. For each size `n`, `n_reps_global` datasets were generated using `simulate_trial_data` (full `freq_vector`). `estimate_effect_misclassify` (with `accuracy=1.0`) was used, effectively analyzing true subgroups post-hoc.
    -   **Aim 2:** Fixed total size `n_fixed_aim2`. Iterated through `accuracy_levels`. For each accuracy, `n_reps_global` datasets were generated using `simulate_trial_data`. `estimate_effect_misclassify` was called for each dataset with the corresponding accuracy.
    -   **Aim 3:** Iterated through `target_groups_aim3`, `accuracy_levels`, and `screen_sizes_aim3`. For each combination, `n_reps_global` replicates were run using `run_enrichment_scenario`, which simulates screening `n_screened` patients, applies the test (`misclassify_group` with accuracy), selects the test-positive cohort, and analyzes it using `fit_glm_safe`.
-   **Statistical Analysis:** Within each simulation replicate and relevant subgroup/cohort, the treatment effect was estimated using logistic regression (`glm(success ~ treatment, family = binomial)`). The `fit_glm_safe` function handled potential errors and insufficient data.
-   **Software:** Simulations were performed in R version 4.x.x using the `tidyverse`, `broom`, and `purrr` packages. Parallel processing for Aim 3 was implemented using the `future` and `furrr` packages. Plots were generated using `ggplot2` and `patchwork`, and tables using `gt`.
-   **Replication:** `n_reps_global` was set to 1000 for all main simulations. Reproducibility was ensured using `set.seed()` globally and managing seeds within loops and parallel processes.

## Performance Measures

The following performance measures were calculated by aggregating results across the `n_reps_global` replicates for each scenario:

-   **Power:** Mean indicator of (p-value \< alpha). Alpha was 0.01 (Bonferroni) for Aims 1 & 2 subgroup analyses, 0.05 for Aim 2 overall analysis, and 0.05 for Aim 3 enrichment analysis.
-   **Bias:** Mean (estimated log(OR) - true target log(OR)).
-   **MSE:** Mean ((estimated log(OR) - true target log(OR))\^2).
-   **Wrong Direction Rate (%):** Mean indicator of (sign(estimated log(OR)) != sign(true target log(OR))) \* 100. (Calculated only for subgroups with true OR != 1).
-   **NNS (Aim 3):** Smallest `n_screened` achieving mean power \>= 0.8.
-   **NNR (Aim 3):** Mean `n_randomized_actual` corresponding to the NNS achieving 80% power.

## Results



```{r define_parameters}
# Define parameters based on ARREST trial data
or_mortality       <- c(A = 1.0, B = 18.8, C = 0.79, D = 1.4, E = 0.3)
freq_arrest        <- c(A = 60/388, B = 52/388, C = 138/388, D = 69/388, E = 69/388)
p0_arrest_raw      <- c(A = 7/60, B = 0/52, C = 11/138, D = 11/69, E = 1/69)

# Apply Haldane-Anscombe correction for subgroup B (0 events in control)
n0_B <- 52 # N in control group B from ARREST placebo
p0_B_corrected <- 0.5 / (n0_B + 0.5) # Corrected baseline probability
p0_arrest_adjusted <- p0_arrest_raw
p0_arrest_adjusted["B"] <- p0_B_corrected # Use corrected p0 for B (~0.0095)

# Simulation settings
n_reps_global      <- 1000 # Final desired number of reps (increased from 10 for better stability)
sample_sizes       <- c(500, 1000, 2000, 3000, 5000, 7500, 10000, 15000, 20000) # For Aim 1
# Define screening sizes for Aim 3 (needs to cover expected NNS)
screen_sizes_aim3  <- c(1000, 2500, 5000, 7500, 10000, 15000, 20000, 30000, 40000, 50000, 75000, 100000, 150000, 200000)
accuracy_levels    <- seq(0.5, 1.0, by = 0.05) # For Aim 2 & Aim 3 test accuracy
n_fixed_aim2       <- 10000 # Sample size for Aim 2
alpha_bonferroni   <- 0.05 / 5 # Alpha for subgroup analyses (Aims 1 & 2)
alpha_overall      <- 0.05     # Alpha for overall analysis (Aim 2)
alpha_aim3         <- 0.05     # Alpha for enrichment trial analysis (Aim 3)
target_groups_aim3 <- c("B", "C", "E") # Target groups for enrichment

# Display target ORs for ARREST simulation
tibble(
  Subgroup = names(or_mortality),
  `Target OR (Mortality)` = or_mortality
) %>%
  gt() %>%
  tab_header(title = "Target Treatment Effects (ARREST ORs)") %>%
  fmt_number(columns = `Target OR (Mortality)`, decimals = 2)
```



### Aim 1: Sample Size Requirements (Post-Hoc Analysis Power)

Simulations estimated the statistical power for post-hoc subgroup analyses within standard trials of increasing total size (`n_total`), assuming perfect classification.



```{r aim1_simulations}
# Placeholder function if functions.R is not available
# Remove or replace if functions.R is sourced correctly
if (!exists("replicate_sims")) {
    replicate_sims <- function(or_vector, freq_vector, p0_vector, n, accuracy, n_reps, seed) {
        warning("Using placeholder 'replicate_sims'. Ensure functions.R is sourced.", call. = FALSE)
        # Return dummy data structure matching expected output
        groups <- c(names(or_vector), "Overall")
        expand_grid(rep = 1:n_reps, group = groups) %>%
            mutate(
                beta = rnorm(n(), mean=log(1), sd=0.5), # Dummy beta
                se = runif(n(), 0.1, 1), # Dummy SE
                pval = runif(n()) # Dummy p-value
            )
    }
}

# Run simulations for Aim 1 (No Caching)
# Setup parallel processing
n_cores_aim1 <- parallel::detectCores() - 1
if (n_cores_aim1 < 1) n_cores_aim1 <- 1
plan(multisession, workers = n_cores_aim1)
message(paste("Starting Aim 1 simulations using", n_cores_aim1, "cores..."))

results_aim1 <- future_map_dfr(sample_sizes, ~replicate_sims(
  or_vector  = or_mortality,
  freq_vector = freq_arrest, # Full population mix
  p0_vector  = p0_arrest_adjusted,
  n          = .x, # Total trial size
  accuracy   = 1.0, # Perfect classification for Aim 1
  n_reps     = n_reps_global,
  seed       = 1 # Consistent base seed for this aim
), .id = "size_id", .options = furrr_options(seed = TRUE)) %>%
  mutate(n_total = sample_sizes[as.numeric(size_id)])

# Shut down parallel workers
plan(sequential)
message("Aim 1 simulations complete.")

# Calculate power for each subgroup (Bonferroni corrected alpha for post-hoc)
power_aim1 <- results_aim1 %>%
  filter(group != "Overall") %>%
  mutate(pval = as.numeric(pval)) %>% # Ensure pval is numeric
  group_by(n_total, group) %>%
  summarise(power = mean(pval < alpha_bonferroni, na.rm = TRUE), .groups = "drop")

# Display power table
#| label: tbl-aim1-table-power
#| tbl-cap: "Aim 1: Power for Post-Hoc Subgroup Analysis vs. Total Trial Size (ARREST ORs, Perfect Classification)"
power_aim1 %>%
  mutate(power = scales::percent(power, accuracy = 1)) %>%
  pivot_wider(names_from = group, values_from = power) %>%
  gt() %>%
  cols_label(n_total = "Total Sample Size") %>%
  # tab_header(title = "Aim 1: Power for Post-Hoc Subgroup Analysis vs. Total Trial Size (ARREST ORs, Perfect Classification)") %>% # Redundant with caption
  tab_footnote(footnote = paste("Power calculated at Bonferroni-corrected alpha =", round(alpha_bonferroni, 3))) %>%
  fmt_percent(columns = where(is.character), scale_values = FALSE, decimals = 1) # Apply fmt_percent to character cols (pivoted power)

# Display power plot
#| label: fig-aim1-plot-power
#| fig-cap: "Aim 1: Power for Post-Hoc Subgroup Analysis vs. Total Trial Size (ARREST ORs)"
ggplot(power_aim1, aes(x = n_total, y = power, color = group)) +
  geom_line() + geom_point() +
  geom_hline(yintercept = 0.8, linetype = "dashed") +
  scale_x_continuous(breaks = sample_sizes, labels = scales::comma) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    # title = "Aim 1: Power for Post-Hoc Subgroup Analysis vs. Total Trial Size (ARREST ORs)", # Redundant with caption
    x = "Total Trial Sample Size",
    y = "Power (Bonferroni Corrected)",
    color = "Subgroup"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



As shown in @fig-aim1-plot-power and @tbl-aim1-table-power, achieving 80% power for post-hoc analyses required very large total trial sizes, exceeding 20,000 for subgroups B, C, and E, even with perfect classification. This was particularly pronounced for subgroup B, where power remained low despite the large OR, likely due to the low baseline event rate.

### Aim 2: Impact of Classification Accuracy (Post-Hoc Analysis)

Simulations assessed the impact of varying classification accuracy on post-hoc analyses in a trial with N = `r n_fixed_aim2`.



```{r aim2_simulations}
# Run simulations for Aim 2 (No Caching)
# Setup parallel processing
n_cores_aim2 <- parallel::detectCores() - 1
if (n_cores_aim2 < 1) n_cores_aim2 <- 1
plan(multisession, workers = n_cores_aim2)
message(paste("Starting Aim 2 simulations using", n_cores_aim2, "cores..."))

results_aim2 <- future_map_dfr(accuracy_levels, ~replicate_sims(
  or_vector  = or_mortality,
  freq_vector = freq_arrest, # Full population mix
  p0_vector  = p0_arrest_adjusted,
  n          = n_fixed_aim2, # Fixed total trial size
  accuracy   = .x,           # Varying accuracy
  n_reps     = n_reps_global,
  seed       = 2 # Consistent base seed for this aim
), .id = "acc_id", .options = furrr_options(seed = TRUE)) %>%
  mutate(accuracy = accuracy_levels[as.numeric(acc_id)])

# Shut down parallel workers
plan(sequential)
message("Aim 2 simulations complete.")


# Define the correct target betas (log ORs) for ARREST scenario
true_betas_arrest <- tibble(group = names(or_mortality), true_beta_target = log(or_mortality))
# Calculate expected overall beta (weighted average of log ORs)
# Note: This is an approximation, true overall effect depends on how misclassification impacts group sizes
true_beta_overall_arrest <- sum(log(or_mortality) * freq_arrest, na.rm = TRUE)
true_betas_arrest <- bind_rows(true_betas_arrest, tibble(group="Overall", true_beta_target = true_beta_overall_arrest))

# Join target betas and calculate per-replicate performance metrics
results_aim2_processed <- results_aim2 %>%
  mutate(beta = as.numeric(beta), pval = as.numeric(pval)) %>% # Ensure numeric types
  left_join(true_betas_arrest, by = "group") %>%
  mutate(
    # Determine significance based on group (subgroup vs overall alpha)
    is_significant = if_else(group == "Overall", pval < alpha_overall, pval < alpha_bonferroni),
    bias_val = beta - true_beta_target,
    mse_val = (beta - true_beta_target)^2,
    # Wrong direction if signs differ (handle true OR=1 case where target beta=0)
    wrong_dir_val = if_else(true_beta_target == 0, sign(beta) != 0, sign(beta) != sign(true_beta_target))
  )

# Summarise performance metrics across replicates
summary_aim2 <- results_aim2_processed %>%
  group_by(accuracy, group) %>%
  summarise(
    power     = mean(is_significant, na.rm = TRUE),
    bias      = mean(bias_val, na.rm = TRUE),
    mse       = mean(mse_val, na.rm = TRUE),
    wrong_dir = mean(wrong_dir_val, na.rm = TRUE) * 100, # As percentage
    n_reps_successful = sum(!is.na(pval)), # Count non-NA p-values
    .groups   = "drop"
  ) %>%
  # Ensure all combinations of accuracy and group are present
  complete(accuracy, group = c("A", "B", "C", "D", "E", "Overall")) %>%
  arrange(group, accuracy)

# Display summary table (long format)
#| label: tbl-aim2-table-summary-long
#| tbl-cap: "Aim 2: Post-Hoc Analysis Summary vs. Accuracy (ARREST ORs, Total N = 10,000)"
summary_aim2 %>%
  select(
    group,
    accuracy,
    Power = power,
    Bias = bias,
    MSE = mse,
    `Wrong Dir %` = wrong_dir
  ) %>%
  gt(groupname_col = "group") %>%
  fmt_percent(columns = Power, decimals = 1) %>%
  fmt_number(columns = c(Bias, MSE), decimals = 2) %>%
  fmt_percent(columns = `Wrong Dir %`, scale_values = FALSE, decimals = 1) %>%
  fmt_number(columns = accuracy, decimals = 2) %>%
  cols_label(
    accuracy = "Accuracy",
    Power = "Power",
    Bias = "Bias",
    MSE = "MSE",
    `Wrong Dir %` = "Wrong Dir %"
  ) %>%
  # Removed header - caption serves as title
  # tab_header(title = paste("Aim 2: Post-Hoc Analysis Summary vs. Accuracy (ARREST ORs, Total N =", n_fixed_aim2, ")")) %>%
  tab_options(row_group.font.weight = "bold")

# Plot Power vs Accuracy
#| label: fig-aim2-plot-power
#| fig-cap: "Aim 2: Post-Hoc Analysis Power vs Classification Accuracy (ARREST ORs, N = 10,000)"
ggplot(summary_aim2, aes(x = accuracy, y = power, color = group)) +
  geom_line() + geom_point() +
  geom_hline(yintercept = 0.8, linetype = "dashed") +
  scale_y_continuous(labels = scales::percent) +
  labs(
    # title = paste("Aim 2: Post-Hoc Analysis Power vs Accuracy (ARREST ORs, N =", n_fixed_aim2, ")"), # Redundant
    x     = "Classification Accuracy",
    y     = "Power",
    color = "Subgroup/Overall"
  )

# Plot Bias vs Accuracy
#| label: fig-aim2-plot-bias
#| fig-cap: "Aim 2: Post-Hoc Estimation Bias vs Classification Accuracy (ARREST ORs, N = 10,000)"
# Filter out Overall group for this plot as bias interpretation differs
ggplot(summary_aim2 %>% filter(group != "Overall"), aes(x = accuracy, y = bias, color = group)) +
  geom_line() + geom_point() +
  facet_wrap(~ group, scales = "free_y") + # Facet by group with free y-scales
  geom_hline(yintercept = 0, linetype = "dashed") + # Line at zero bias
  labs(
    # title = paste("Aim 2: Post-Hoc Estimation Bias vs Accuracy (ARREST ORs, N =", n_fixed_aim2, ")"), # Redundant
    x     = "Classification Accuracy",
    y     = "Bias (log OR)",
    color = "Subgroup"
  )

# Plot Wrong Direction Rate vs Accuracy
#| label: fig-aim2-plot-wrong-direction
#| fig-cap: "Aim 2: Post-Hoc Wrong Direction % vs Classification Accuracy (ARREST ORs, N = 10,000)"
# Filter out Overall and Group A (true OR=1) for this plot
ggplot(summary_aim2 %>% filter(group != "Overall", group != "A"), aes(x = accuracy, y = wrong_dir, color = group)) +
  geom_line() + geom_point() +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) + # Format y-axis as percentage
  labs(
    # title = paste("Aim 2: Post-Hoc Wrong Direction % vs Accuracy (ARREST ORs, N =", n_fixed_aim2, ")"), # Redundant
    subtitle = "Group A (True OR=1) excluded as 'wrong direction' is ill-defined.",
    x     = "Classification Accuracy",
    y     = "Wrong Direction (%)",
    color = "Subgroup"
  )
```



Decreasing classification accuracy substantially reduced power (@fig-aim2-plot-power), increased bias towards the null (@fig-aim2-plot-bias), increased MSE, and increased the probability of estimating effects in the wrong direction (@fig-aim2-plot-wrong-direction), particularly for subgroups with true effects further from the null (B and E). Full results are in @tbl-aim2-table-summary-long and Supplementary Figure (@fig-supp-bias-arrest).

### Aim 3: Sample Size Requirements (Enrichment Trial Simulation)

This section estimates the Number Needed to Screen (NNS) and the average resulting Number Needed to Randomize (NNR) to achieve 80% power in an **enrichment trial**, where recruitment is guided by a test with varying accuracy.



```{r aim3_enrichment_simulations}
# Function to run enrichment simulation replicates for one scenario
run_enrichment_scenario <- function(n_screened, target_group, accuracy,
                                    or_vector, p0_vector, freq_vector,
                                    n_reps, base_seed) {
  # Ensure freq_vector has names
  if(is.null(names(freq_vector))) names(freq_vector) <- LETTERS[1:length(freq_vector)]

  map_dfr(1:n_reps, ~{
    # Generate unique seed for this replicate
    seed <- base_seed + .x

    # 1. Simulate screened cohort using the full population mix
    screened_data <- simulate_trial_data(
      or_vector = or_vector,
      freq_vector = freq_vector, # Use the full population frequency
      n = n_screened,
      p0_vector = p0_vector,
      seed = seed
    )

    # 2. Apply test (misclassify) to the screened cohort
    # Assumes misclassify_group function is available (defined in setup)
    tested_data <- misclassify_group(
        screened_data,
        accuracy = accuracy,
        freq_vector = freq_vector, # Pass named freq_vector for sampling if incorrect
        seed = seed + n_reps # Use different seed offset for misclassification step
        )

    # 3. Select cohort for randomization ("test positives" for the target group)
    randomized_cohort <- tested_data %>% filter(assigned_group == target_group)
    n_randomized_actual <- nrow(randomized_cohort)

    # 4. Analyze the randomized cohort if it's large enough
    if(n_randomized_actual >= 10) { # Need minimum size for GLM stability
       analysis_results <- fit_glm_safe(randomized_cohort)
    } else {
       # If too small, return NA results
       analysis_results <- tibble(beta = NA_real_, se = NA_real_, pval = NA_real_)
    }

    # Return results for this replicate, including actual N randomized
    tibble(
        n_randomized_actual = n_randomized_actual
        ) %>% bind_cols(analysis_results)

  }, .id = "rep")
}

# Placeholder function if simulate_trial_data is not available from functions.R
if (!exists("simulate_trial_data")) {
    simulate_trial_data <- function(or_vector, freq_vector, n, p0_vector, seed = NULL) {
        warning("Using placeholder 'simulate_trial_data'. Ensure functions.R is sourced.", call. = FALSE)
        if(!is.null(seed)) set.seed(seed)
        # Return dummy data structure
        groups <- names(freq_vector)
        if(is.null(groups)) groups <- LETTERS[1:length(freq_vector)]
        tibble(
            id = 1:n,
            group = sample(groups, n, replace = TRUE, prob = freq_vector),
            treatment = rbinom(n, 1, 0.5),
            success = rbinom(n, 1, 0.2) # Dummy outcome
        )
    }
}


# --- Run Aim 3 Simulations ---
# Define parameter grid for Aim 3
aim3_params <- expand_grid(
    target_group = target_groups_aim3,
    accuracy = accuracy_levels, # Use same accuracy levels as Aim 2
    n_screened = screen_sizes_aim3
)

# Setup parallel processing using future and furrr
n_cores_aim3 <- parallel::detectCores() - 1
if (n_cores_aim3 < 1) n_cores_aim3 <- 1
plan(multisession, workers = n_cores_aim3)
message(paste("Starting Aim 3 simulations using", n_cores_aim3, "cores..."))

# Use future_pmap_dfr for parallel execution over the parameter grid
results_aim3 <- future_pmap_dfr(aim3_params, function(target_group, accuracy, n_screened) {
    # message(paste("Running Aim 3: Group", target_group, "Acc", accuracy, "N_screen", n_screened)) # Optional progress message
    run_enrichment_scenario(
        n_screened = n_screened,
        target_group = target_group,
        accuracy = accuracy,
        or_vector = or_mortality,       # ARREST ORs
        p0_vector = p0_arrest_adjusted, # ARREST baseline risks
        freq_vector = freq_arrest,      # ARREST frequencies
        n_reps = n_reps_global,
        # Ensure unique base seed for each scenario combination to avoid seed collision
        base_seed = 3000 + which(aim3_params$target_group == target_group & aim3_params$accuracy == accuracy & aim3_params$n_screened == n_screened) * n_reps_global
        ) %>%
    mutate( # Add parameters back for easy grouping later
        target_group = target_group,
        accuracy = accuracy,
        n_screened = n_screened
        )
}, .progress = TRUE, .options = furrr_options(seed = TRUE)) # Use progress bar and ensure seeds work correctly in parallel

# Shut down parallel workers
plan(sequential)
message("Aim 3 simulations complete. Processing results...")

# --- Process Aim 3 Results ---
# Calculate power and average N randomized for each scenario
summary_aim3 <- results_aim3 %>%
  mutate(pval = as.numeric(pval)) %>% # Ensure numeric
  group_by(target_group, accuracy, n_screened) %>%
  summarise(
    power = mean(pval < alpha_aim3, na.rm = TRUE), # Power at alpha=0.05 for enrichment
    mean_n_randomized = mean(n_randomized_actual, na.rm = TRUE),
    sd_n_randomized = sd(n_randomized_actual, na.rm = TRUE),
    n_reps_analyzed = sum(!is.na(pval)), # Count successful analyses
    .groups = "drop"
  )

# Find NNS required for 80% power for each target group and accuracy
nns_enrich <- summary_aim3 %>%
  group_by(target_group, accuracy) %>%
  arrange(n_screened) %>% # Order by N screened
  filter(power >= 0.8) %>% # Find scenarios meeting power threshold
  slice(1) %>% # Take the first (smallest N screened) that meets threshold
  select(target_group, accuracy, nns_needed_num = n_screened) %>% # Select the NNS
  ungroup() %>%
  # Ensure all combinations are present, even if power goal was not met within simulated range
  full_join(expand_grid(target_group = target_groups_aim3, accuracy = accuracy_levels), by = c("target_group", "accuracy")) %>%
  mutate(
    max_n_screened_aim3 = max(screen_sizes_aim3), # Max N screened in simulation
    # Format NNS nicely, indicating if > max simulated
    `N Needed to Screen (NNS)` = if_else(
      is.na(nns_needed_num), # If NA, power goal wasn't met
      paste(">", scales::comma(max_n_screened_aim3)), # Indicate > max
      scales::comma(nns_needed_num) # Format with comma
    )
  ) %>%
  select(target_group, accuracy, `N Needed to Screen (NNS)`, nns_needed_num) # Keep numeric NNS for joining

# Get the corresponding average NNR for the NNS found
nnr_enrich <- nns_enrich %>%
  # Join back to summary_aim3 only where NNS was determined (i.e., nns_needed_num is not NA)
  left_join(summary_aim3 %>% select(target_group, accuracy, n_screened, mean_n_randomized),
            by = c("target_group", "accuracy", "nns_needed_num" = "n_screened")) %>%
  mutate(`NNR (Enrichment)` = if_else(
      is.na(mean_n_randomized), # Check if join failed (i.e., power not reached)
      "-",                      # Indicate NNR not applicable
      scales::comma(round(mean_n_randomized), accuracy = 1) # Format NNR
      )) %>%
  select(target_group, accuracy, `NNR (Enrichment)`) # Select relevant columns


# Combine NNS and NNR results into a final table
implications_enrichment <- nns_enrich %>%
    left_join(nnr_enrich, by = c("target_group", "accuracy")) %>%
    # Add the target OR for context
    left_join(
        tibble(target_group = names(or_mortality), target_or = or_mortality) %>% filter(target_group %in% target_groups_aim3),
        by = "target_group"
    ) %>%
    select(
        Subgroup = target_group,
        `Target OR` = target_or,
        `Test Accuracy` = accuracy,
        `NNR (Enrichment)`, # Average N randomized in enriched cohort
        `N Needed to Screen (NNS)` # N screened from general pop
    ) %>%
    arrange(Subgroup, desc(`Test Accuracy`)) # Order for readability

# Plot power vs N Screened
#| label: fig-aim3-plot-power
#| fig-cap: "Aim 3: Power vs. N Screened in Enrichment Trial (Varying Test Accuracy)"
ggplot(summary_aim3, aes(x = n_screened, y = power, color = factor(accuracy))) +
  geom_line() + geom_point() +
  geom_hline(yintercept = 0.8, linetype = "dashed") + # 80% power line
  facet_wrap(~ target_group, scales = "free_x") + # Separate plot per target group
  scale_x_continuous(labels = scales::comma) + # Format x-axis
  scale_y_continuous(labels = scales::percent) + # Format y-axis
  labs(
    # title = "Aim 3: Power vs. N Screened in Enrichment Trial (Varying Accuracy)", # Redundant
    x = "N Screened",
    y = "Power (alpha = 0.05)",
    color = "Test Accuracy"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```{r aim3_plot_nns}
#| label: fig-aim3-plot-nns
#| fig-cap: "Aim 3: Number Needed to Screen (NNS) vs. Test Accuracy for 80% Power"

# Plot NNS vs Accuracy
# Exclude ">" values for plotting (where power goal wasn't met)
plot_data_nns <- implications_enrichment %>%
  filter(str_detect(`N Needed to Screen (NNS)`, "^>", negate=TRUE)) %>%
  mutate(nns_numeric = as.numeric(gsub(",", "", `N Needed to Screen (NNS)`)))

ggplot(plot_data_nns, aes(x = `Test Accuracy`, y = nns_numeric, color = Subgroup)) +
  geom_line() + geom_point() +
  scale_y_log10(labels = scales::comma) + # Use log scale for NNS due to large range
  labs(title = NULL, #"NNS vs. Test Accuracy", # Redundant
       x = "Test Accuracy", y = "N Needed to Screen (Log Scale)") +
  theme(legend.position = 'bottom')

```



Aim 3 simulated enrichment trials targeting subgroups B, C, and E. Power increased with the number screened, but the required NNS to achieve 80% power was heavily dependent on both the target subgroup and the test accuracy (@fig-aim3-plot-power). As test accuracy decreased, the NNS required to achieve 80% power increased dramatically, often by orders of magnitude (@fig-aim3-plot-nns). The corresponding average NNR within the enriched cohort also increased with lower accuracy due to dilution by false positives (see @tbl-implications-table).

## Supplementary: Realistic Effects

This section repeats the analyses using more plausible or 'realistic' effect sizes.



```{r setup_realistic, include=FALSE}
# Define 'realistic' ORs
or_realistic <- c(A = 1.0, B = 2.0, C = 0.7, D = 1.2, E = 0.8)

# Display target ORs for Realistic simulation
tibble(
  Subgroup = names(or_realistic),
  `Target OR (Mortality)` = or_realistic
) %>%
  gt() %>%
  tab_header(title = "Target Treatment Effects (Realistic ORs)") %>%
  fmt_number(columns = `Target OR (Mortality)`, decimals = 2)
```



### Supplementary Aim 1: Power vs. Sample Size (Realistic ORs)



```{r aim1_simulations_realistic}
# Run simulations for Aim 1 (Realistic)
# Setup parallel processing
n_cores_aim1_real <- parallel::detectCores() - 1
if (n_cores_aim1_real < 1) n_cores_aim1_real <- 1
plan(multisession, workers = n_cores_aim1_real)
message(paste("Starting Realistic Aim 1 simulations using", n_cores_aim1_real, "cores..."))

results_aim1_real <- future_map_dfr(sample_sizes, ~replicate_sims(
  or_vector   = or_realistic,       # Use realistic ORs
  freq_vector = freq_arrest,
  n           = .x,
  p0_vector   = p0_arrest_adjusted, # Same baseline risks
  accuracy    = 1.0,                # Perfect classification
  n_reps      = n_reps_global,
  seed        = 4 # Different base seed for this scenario
), .id = "size_id", .options = furrr_options(seed = TRUE)) %>%
  mutate(n_total = sample_sizes[as.numeric(size_id)])

# Shut down parallel workers
plan(sequential)
message("Realistic Aim 1 simulations complete.")

# Calculate power
power_aim1_real <- results_aim1_real %>%
  filter(group != "Overall") %>%
  mutate(pval = as.numeric(pval)) %>%
  group_by(n_total, group) %>%
  summarise(power = mean(pval < alpha_bonferroni, na.rm = TRUE), .groups = "drop")

# Display power table
#| label: tbl-aim1-table-power-realistic
#| tbl-cap: "Supplementary: Power vs. Sample Size (Realistic ORs, Perfect Classification)"
power_aim1_real %>%
  mutate(power = scales::percent(power, accuracy = 1)) %>%
  pivot_wider(names_from = group, values_from = power) %>%
  gt() %>%
  cols_label(n_total = "Total Sample Size") %>%
  # tab_header(title = "Supplementary: Power vs. Sample Size (Realistic ORs, Perfect Classification)") %>%
  tab_footnote(footnote = paste("Power calculated at Bonferroni-corrected alpha =", round(alpha_bonferroni, 3))) %>%
  fmt_percent(columns = where(is.character), scale_values = FALSE, decimals = 1)

# Display power plot
#| label: fig-aim1-plot-power-realistic
#| fig-cap: "Supplementary: Power vs. Sample Size (Realistic ORs)"
ggplot(power_aim1_real, aes(x = n_total, y = power, color = group)) +
  geom_line() + geom_point() +
  geom_hline(yintercept = 0.8, linetype = "dashed") +
  scale_x_continuous(breaks = sample_sizes, labels = scales::comma) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    # title = "Supplementary: Power vs. Sample Size (Realistic ORs)",
    x     = "Total Sample Size",
    y     = "Power",
    color = "Subgroup"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



### Supplementary Aim 2: Power vs. Accuracy (Realistic ORs)



```{r aim2_simulations_realistic}
# Run simulations for Aim 2 (Realistic)
# Setup parallel processing
n_cores_aim2_real <- parallel::detectCores() - 1
if (n_cores_aim2_real < 1) n_cores_aim2_real <- 1
plan(multisession, workers = n_cores_aim2_real)
message(paste("Starting Realistic Aim 2 simulations using", n_cores_aim2_real, "cores..."))

results_aim2_real <- future_map_dfr(accuracy_levels, ~replicate_sims(
  or_vector   = or_realistic,       # Use realistic ORs
  freq_vector = freq_arrest,
  n           = n_fixed_aim2,       # Fixed N
  p0_vector   = p0_arrest_adjusted,
  accuracy    = .x,                 # Varying accuracy
  n_reps      = n_reps_global,
  seed        = 5 # Different base seed
), .id = "acc_id", .options = furrr_options(seed = TRUE)) %>%
  mutate(accuracy = accuracy_levels[as.numeric(acc_id)])

# Shut down parallel workers
plan(sequential)
message("Realistic Aim 2 simulations complete.")

# Define and join correct target betas for realistic ORs
true_betas_realistic <- tibble(group = names(or_realistic), true_beta_target = log(or_realistic))
true_beta_overall_realistic <- sum(log(or_realistic) * freq_arrest, na.rm = TRUE) # Approx overall
true_betas_realistic <- bind_rows(true_betas_realistic, tibble(group="Overall", true_beta_target = true_beta_overall_realistic))

# Join target betas and calculate per-replicate metrics
results_aim2_real_processed <- results_aim2_real %>%
  mutate(beta = as.numeric(beta), pval = as.numeric(pval)) %>%
  left_join(true_betas_realistic, by = "group") %>%
  mutate(
    is_significant = if_else(group == "Overall", pval < alpha_overall, pval < alpha_bonferroni),
    bias_val = beta - true_beta_target,
    mse_val = (beta - true_beta_target)^2,
    wrong_dir_val = if_else(true_beta_target == 0, sign(beta) != 0, sign(beta) != sign(true_beta_target))
  )

# Summarise across replicates
summary_aim2_real <- results_aim2_real_processed %>%
  group_by(accuracy, group) %>%
  summarise(
    power     = mean(is_significant, na.rm = TRUE),
    bias      = mean(bias_val, na.rm = TRUE),
    mse       = mean(mse_val, na.rm = TRUE),
    wrong_dir = mean(wrong_dir_val, na.rm = TRUE) * 100,
    .groups   = "drop"
  ) %>%
  complete(accuracy, group = c("A", "B", "C", "D", "E", "Overall")) %>%
  arrange(group, accuracy)

# Display summary table (long format)
#| label: tbl-aim2-table-summary-long-realistic
#| tbl-cap: "Supplementary: Simulation Summary vs. Accuracy (Realistic ORs, N = 10,000)"
summary_aim2_real %>%
  select(
    group,
    accuracy,
    Power = power,
    Bias = bias,
    MSE = mse,
    `Wrong Dir %` = wrong_dir
  ) %>%
  gt(groupname_col = "group") %>%
  fmt_percent(columns = Power, decimals = 1) %>%
  fmt_number(columns = c(Bias, MSE), decimals = 2) %>%
  fmt_percent(columns = `Wrong Dir %`, scale_values = FALSE, decimals = 1) %>%
  fmt_number(columns = accuracy, decimals = 2) %>%
  cols_label(
    accuracy = "Accuracy",
    Power = "Power",
    Bias = "Bias",
    MSE = "MSE",
    `Wrong Dir %` = "Wrong Dir %"
  ) %>%
  # tab_header(title = paste("Supplementary: Simulation Summary vs. Accuracy (Realistic ORs, N =", n_fixed_aim2, ")")) %>%
  tab_options(row_group.font.weight = "bold")

# Plot Power vs Accuracy
#| label: fig-aim2-plot-power-realistic
#| fig-cap: "Supplementary: Power vs Accuracy (Realistic ORs, N = 10,000)"
ggplot(summary_aim2_real, aes(x = accuracy, y = power, color = group)) +
  geom_line() + geom_point() +
  geom_hline(yintercept = 0.8, linetype = "dashed") +
  scale_y_continuous(labels = scales::percent) +
  labs(
    # title = paste("Supplementary: Power vs Accuracy (Realistic; n =", n_fixed_aim2, ")"),
    x     = "Classification Accuracy",
    y     = "Power",
    color = "Subgroup/Overall"
  )

# Plot Bias vs Accuracy
#| label: fig-aim2-plot-bias-realistic
#| fig-cap: "Supplementary: Estimation Bias vs Accuracy (Realistic ORs, N = 10,000)"
# Filter out Overall group
ggplot(summary_aim2_real %>% filter(group != "Overall"), aes(x = accuracy, y = bias, color = group)) +
  geom_line() + geom_point() +
  facet_wrap(~ group, scales = "free_y") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    # title = paste("Supplementary: Estimation Bias vs Accuracy (Realistic; n =", n_fixed_aim2, ")"),
    x     = "Classification Accuracy",
    y     = "Bias (log OR)",
    color = "Subgroup"
  )

# Plot Wrong Direction Rate vs Accuracy
#| label: fig-aim2-plot-wrong-direction-realistic
#| fig-cap: "Supplementary: Wrong Direction % vs Accuracy (Realistic ORs, N = 10,000)"
# Filter out Overall and Group A
ggplot(summary_aim2_real %>% filter(group != "Overall", group != "A"), aes(x = accuracy, y = wrong_dir, color = group)) +
  geom_line() + geom_point() +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  labs(
    # title = paste("Supplementary: Wrong Direction % vs Accuracy (Realistic; n =", n_fixed_aim2, ")"),
    subtitle = "Group A (True OR=1) excluded as 'wrong direction' is ill-defined.",
    x     = "Classification Accuracy",
    y     = "Wrong Direction (%)",
    color = "Subgroup"
  )
```



### Supplementary Aim 3: Enrichment Trial Simulation (Realistic ORs)

This section repeats the Aim 3 enrichment trial simulation using the 'realistic' ORs.



```{r aim3_enrichment_simulations_realistic}
# --- Run Aim 3 Simulations (Realistic ORs) ---
# Define parameter grid for Aim 3 Realistic
aim3_params_real <- expand_grid(
    target_group = target_groups_aim3, # Same target groups B, C, E
    accuracy = accuracy_levels,
    n_screened = screen_sizes_aim3
)

# Setup parallel processing
n_cores_aim3_real <- parallel::detectCores() - 1
if (n_cores_aim3_real < 1) n_cores_aim3_real <- 1
plan(multisession, workers = n_cores_aim3_real)
message(paste("Starting Realistic Aim 3 simulations using", n_cores_aim3_real, "cores..."))

# Use future_pmap_dfr for parallel execution
results_aim3_real <- future_pmap_dfr(aim3_params_real, function(target_group, accuracy, n_screened) {
    # message(paste("Running Realistic Aim 3: Group", target_group, "Acc", accuracy, "N_screen", n_screened)) # Optional progress
    run_enrichment_scenario(
        n_screened = n_screened,
        target_group = target_group,
        accuracy = accuracy,
        or_vector = or_realistic, # Use realistic ORs
        p0_vector = p0_arrest_adjusted,
        freq_vector = freq_arrest,
        n_reps = n_reps_global,
        # Unique base seed
        base_seed = 4000 + which(aim3_params_real$target_group == target_group & aim3_params_real$accuracy == accuracy & aim3_params_real$n_screened == n_screened) * n_reps_global
        ) %>%
    mutate( # Add parameters back
        target_group = target_group,
        accuracy = accuracy,
        n_screened = n_screened
        )
}, .progress = TRUE, .options = furrr_options(seed = TRUE))

# Shut down parallel workers
plan(sequential)
message("Realistic Aim 3 simulations complete. Processing results...")

# --- Process Aim 3 Realistic Results ---
summary_aim3_real <- results_aim3_real %>%
  mutate(pval = as.numeric(pval)) %>%
  group_by(target_group, accuracy, n_screened) %>%
  summarise(
    power = mean(pval < alpha_aim3, na.rm = TRUE),
    mean_n_randomized = mean(n_randomized_actual, na.rm = TRUE),
    sd_n_randomized = sd(n_randomized_actual, na.rm = TRUE),
    n_reps_analyzed = sum(!is.na(pval)),
    .groups = "drop"
  )

# Find NNS required for 80% power (Realistic)
nns_enrich_real <- summary_aim3_real %>%
  group_by(target_group, accuracy) %>%
  arrange(n_screened) %>%
  filter(power >= 0.8) %>%
  slice(1) %>%
  select(target_group, accuracy, nns_needed_num = n_screened) %>%
  ungroup() %>%
  full_join(expand_grid(target_group = target_groups_aim3, accuracy = accuracy_levels), by = c("target_group", "accuracy")) %>%
  mutate(
    max_n_screened_aim3 = max(screen_sizes_aim3),
    `N Needed to Screen (NNS)` = if_else(
      is.na(nns_needed_num),
      paste(">", scales::comma(max_n_screened_aim3)),
      scales::comma(nns_needed_num)
    )
  ) %>%
  select(target_group, accuracy, `N Needed to Screen (NNS)`, nns_needed_num)

# Get corresponding average NNR (Realistic)
nnr_enrich_real <- nns_enrich_real %>%
  left_join(summary_aim3_real %>% select(target_group, accuracy, n_screened, mean_n_randomized),
            by = c("target_group", "accuracy", "nns_needed_num" = "n_screened")) %>%
  mutate(`NNR (Enrichment)` = if_else(
      is.na(mean_n_randomized), "-", scales::comma(round(mean_n_randomized), accuracy = 1)
      )) %>%
  select(target_group, accuracy, `NNR (Enrichment)`)

# Combine NNS and NNR results (Realistic)
implications_enrichment_real <- nns_enrich_real %>%
    left_join(nnr_enrich_real, by = c("target_group", "accuracy")) %>%
    left_join(
        tibble(target_group = names(or_realistic), target_or = or_realistic) %>% filter(target_group %in% target_groups_aim3),
        by = "target_group"
    ) %>%
    select(
        Subgroup = target_group,
        `Target OR` = target_or,
        `Test Accuracy` = accuracy,
        `NNR (Enrichment)`,
        `N Needed to Screen (NNS)`
    ) %>%
    arrange(Subgroup, desc(`Test Accuracy`))

# Display NNS/NNR table (Realistic)
#| label: tbl-implications-table-realistic
#| tbl-cap: "Supplementary: Estimated Sample Sizes for Enrichment Trial (80% Power, Realistic ORs)"
implications_enrichment_real %>%
  gt(groupname_col = c("Subgroup", "Target OR")) %>%
  fmt_number(columns = `Test Accuracy`, decimals = 2) %>%
  cols_align(align = "right", columns = contains("NNR") | contains("NNS")) %>%
  cols_label(
     `Test Accuracy` = "Test Accuracy",
     `NNR (Enrichment)` = "Avg. NNR",
     `N Needed to Screen (NNS)` = "NNS"
  ) %>%
  # tab_header(title = "Supplementary: Estimated Sample Sizes for Enrichment Trial (80% Power, Realistic ORs)") %>%
  tab_footnote(footnote = "NNR (Enrichment): Average number needed to randomize *within the enriched cohort*. NNS: Number needed to screen in the general population.")

```{r aim3_plot_power_realistic}
#| label: fig-aim3-plot-power-realistic
#| fig-cap: "Supplementary Aim 3: Power vs. N Screened in Enrichment Trial (Realistic ORs, Varying Test Accuracy)"

ggplot(summary_aim3_real, aes(x = n_screened, y = power, color = factor(accuracy))) +
  geom_line() + geom_point() +
  geom_hline(yintercept = 0.8, linetype = "dashed") +
  facet_wrap(~ target_group, scales = "free_x") +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "N Screened",
    y = "Power (alpha = 0.05)",
    color = "Test Accuracy"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```{r aim3_plot_nnr_nns_realistic}
#| label: fig-aim3-plot-nnr-nns-realistic
#| fig-cap: "Supplementary Aim 3: Estimated NNS and Average NNR vs. Test Accuracy for 80% Power (Realistic ORs)"
#| fig-subcap:
#|   - "Average NNR vs. Test Accuracy"
#|   - "NNS vs. Test Accuracy (Log Scale)"

# Plot NNR vs Accuracy (Realistic)
# Filter out "-" values where NNR wasn't calculated
plot_data_nnr_real <- implications_enrichment_real %>%
  filter(`NNR (Enrichment)` != "-") %>%
  mutate(nnr_numeric = as.numeric(gsub(",", "", `NNR (Enrichment)`)))

plot_nnr_real <- ggplot(plot_data_nnr_real,
       aes(x = `Test Accuracy`, y = nnr_numeric, color = Subgroup)) +
  geom_line() + geom_point() +
  scale_y_continuous(labels = scales::comma) + # Use standard scale for NNR
  labs(x = "Test Accuracy", y = "Avg. N Randomized") +
  theme(legend.position = "none") # Remove legend for individual plot

# Plot NNS vs Accuracy (Realistic)
# Filter out ">" values
plot_data_nns_real <- implications_enrichment_real %>%
  filter(str_detect(`N Needed to Screen (NNS)`, "^>", negate=TRUE)) %>%
  mutate(nns_numeric = as.numeric(gsub(",", "", `N Needed to Screen (NNS)`)))

plot_nns_real <- ggplot(plot_data_nns_real,
       aes(x = `Test Accuracy`, y = nns_numeric, color = Subgroup)) +
  geom_line() + geom_point() +
  scale_y_log10(labels = scales::comma) + # Use log scale for NNS
  labs(x = "Test Accuracy", y = "N Needed to Screen (Log Scale)") +
  theme(legend.position = "none") # Remove legend for individual plot

# Combine plots using patchwork, collecting legends at the bottom
plot_nnr_real + plot_nns_real + patchwork::plot_layout(guides = 'collect') & theme(legend.position = 'bottom')

```



## Discussion

This simulation study demonstrates the significant challenges in detecting heterogeneous treatment effects in the presence of subgroup misclassification, using parameters derived from a real-world *S. aureus* bacteremia trial [@Swets2024].

Aim 1 highlights that even with perfect classification, substantial sample sizes are required to achieve adequate power (80%) for **post-hoc subgroup analyses** within a standard trial randomizing the full population mix (@fig-aim1-plot-power). This is particularly true for less prevalent subgroups or those with smaller effect sizes, or where baseline event rates are low (even if the relative effect is large, as seen for subgroup B with the ARREST parameters). The need for multiple comparison adjustments (e.g., Bonferroni) further inflates sample size requirements for post-hoc analyses [@Wang2007].

Aim 2 quantifies the detrimental impact of misclassification accuracy on these post-hoc analyses (@fig-aim2-plot-power, @fig-aim2-plot-bias, @fig-aim2-plot-wrong-direction). As accuracy decreases, statistical power diminishes substantially, bias in effect estimates increases (generally towards the overall null effect), and the probability of estimating effects in the wrong direction rises. This underscores the critical importance of highly accurate subgroup classification methods if relying on post-hoc analyses and aligns with broader concerns about the reliability of subgroup findings [@Pocock2007; Kent2018]. Subgroups with true null effects (like A) are particularly susceptible to high rates of "wrong direction" findings under misclassification.

### Practical Implications: Enrichment Trials

Aim 3 simulates an **enrichment trial** design, where patients are screened using a test with a given `accuracy` (probability of correct classification), and only those testing positive for the target subgroup are randomized. This allows estimation of the Number Needed to Screen (NNS) and the average resulting Number Needed to Randomize (NNR<sub>Enrich</sub>) within the enriched cohort required to achieve 80% power.



```{r practical_implications_table, echo=FALSE}
#| label: tbl-implications-table
#| tbl-cap: "Estimated Sample Sizes for Enrichment Trial targeting specific subgroups with 80% Power (ARREST ORs)"

# Display table using gt
implications_enrichment %>%
  gt(groupname_col = c("Subgroup", "Target OR")) %>%
  fmt_number(columns = `Test Accuracy`, decimals = 2) %>%
  cols_align(align = "right", columns = contains("NNR") | contains("NNS")) %>%
  cols_label(
     `Test Accuracy` = "Test Accuracy",
     `NNR (Enrichment)` = "Avg. NNR",
     `N Needed to Screen (NNS)` = "NNS"
  ) %>%
  # Removed header, caption is sufficient
  # tab_header(title = "Estimated Sample Sizes for Enrichment Trial (80% Power)") %>%
  tab_footnote(footnote = "NNR (Enrichment): Average number needed to randomize *within the enriched cohort* (those testing positive) to achieve 80% power (alpha=0.05), based on test accuracy (from Aim 3). NNS: Number needed to screen in the general population to achieve 80% power.")

```



The results (@tbl-implications-table, @fig-aim3-plot-power, @fig-aim3-plot-nns) demonstrate the trade-offs inherent in enrichment designs [@Simon2004; Antoniou2016]. While potentially requiring fewer randomized patients (NNR) compared to the total N needed for post-hoc power (Aim 1), the screening burden (NNS) can be substantial and increases dramatically as test accuracy decreases. For example, achieving 80% power for subgroup B (OR=18.8) requires screening tens of thousands even with high accuracy, due to its prevalence and the corrected baseline risk. Lower accuracy inflates NNR (due to dilution by false positives) and NNS (due to lower yield). This highlights that the feasibility of enrichment trials depends critically on subgroup prevalence, effect size, and, crucially, the performance characteristics of the screening test [@Anthenelli2011; Wang2014]. Our simulations using more realistic ORs show lower, more achievable NNR/NNS values (see @tbl-implications-table-realistic and @fig-aim3-plot-nnr-nns-realistic), but the strong dependence on accuracy remains.

## Conclusion

This simulation study underscores the importance of considering classification accuracy when interpreting subgroup analyses or planning trials aimed at detecting HTE in heterogeneous diseases like SAB. Post-hoc subgroup analyses require large sample sizes and high classification accuracy to yield reliable results. Enrichment trial designs offer potential efficiency gains in terms of randomized patients but necessitate careful evaluation of the screening burden, which is highly sensitive to test accuracy and subgroup prevalence. Developing and validating accurate methods for identifying SAB subphenotypes is crucial for advancing stratified medicine approaches in this field.

## References

*(Ensure references.bib and vancouver.csl are in the same directory or provide correct paths)*

``` bib
@article{Swets2024,
  author = {Swets, Maaike C and Bakk, Zsuzsa and Westgeest, Annette C and Berry, Karla and Cooper, George and Sim, Wynne and Lee, Rui Shian and Gan, Tze Yi and Donlon, William and Besu, Antonia and Heppenstall, Ellen and Tysall, Lauren and Dewar, Scott and de Boer, Mark G J and Fowler, Jr, Vance G and Dockrell, David H and Thwaites, Guy E and Pujol, Miquel and Pallares, Nuria and Tebe, Cristòfol and Carratalà, Jordi and Szubert, Alan J and Groeneveld, G H Rolf and Russell, Clark D},
  year = {2024},
  month = {06},
  pages = {1153-1161},
  title = {Clinical Subphenotypes of Staphylococcus aureus Bacteremia},
  volume = {79},
  journal = {Clinical Infectious Diseases},
  doi = {10.1093/cid/ciae338}
}

@article{Morris2019,
    author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
    title = "{Using simulation studies to evaluate statistical methods}",
    journal = {Statistics in Medicine},
    volume = {38},
    number = {11},
    pages = {2074-2102},
    keywords = {Monte Carlo, reporting guideline, simulation study, statistical methods},
    doi = {https://doi.org/10.1002/sim.8086},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8086},
    year = {2019}
}

@article{Siepe2024,
    year = {2024},
    author = {Björn S. Siepe and František Bartoš and Tim P. Morris and Anne-Laure Boulesteix and Daniel W. Heck and Samuel Pawel},
    title = {Simulation Studies for Methodological Research in Psychology: A Standardized Structure for Planning, Preregistration, and Reporting},
    doi = {10.1037/met0000695},
    url = {https://doi.org/10.1037/met0000695},
    journal = {Psychological Methods}
}

@article{Tong2015,
    author = {Tong, Steven Y. C. and Davis, Joshua S. and Eichenberger, Emily and Holland, Thomas L. and Fowler, Vance G.},
    title = "{Staphylococcus aureus Infections: Epidemiology, Pathophysiology, Clinical Manifestations, and Management}",
    journal = {Clinical Microbiology Reviews},
    volume = {28},
    number = {3},
    pages = {603-661},
    year = {2015},
    doi = {10.1128/CMR.00134-14},
    url = {https://journals.asm.org/doi/abs/10.1128/CMR.00134-14}
}

@article{GBD2019,
    author = {{GBD 2019 Antimicrobial Resistance Collaborators}},
    title = "{Global mortality associated with 33 bacterial pathogens in 2019: a systematic analysis for the Global Burden of Disease Study 2019}",
    journal = {The Lancet},
    volume = {400},
    number = {10369},
    pages = {2221-2248},
    year = {2022},
    doi = {10.1016/S0140-6736(22)02185-7}
}

@article{Thwaites2018,
    author = {Thwaites, Guy E. and Scarborough, Matthew and Szubert, Alan and Nsutebu, Emmanuel and Tilley, Richard and Greig, Jane and Wyllie, Sarah A. and Wilson, Peter and Auckland, Chloë and Cairns, John and Ward, Debbi and Lal, Punam and Barlow, Gavin and Hopkins, Susan and Gkrania-Klotsas, Effrossyni Z. and Shankaran, Padmasarda and Cripps, Natasha and Davies, Jonathan and Harvey, David and Gubbay, Andrew J. and Klein, J. Louis and Bradley, Chris and Morgan, Mari and Llewelyn, Martin J. and Edgeworth, Jonathan D. and Walker, A. Sarah},
    title = "{Adjunctive rifampicin for Staphylococcus aureus bacteraemia (ARREST): a multicentre, randomised, double-blind, placebo-controlled trial}",
    journal = {The Lancet},
    volume = {391},
    number = {10121},
    pages = {668-678},
    year = {2018},
    doi = {10.1016/S0140-6736(17)32446-X}
}

@article{Holland2022,
    author = {Holland, Thomas L. and Bayer, Arnold S. and Fowler, Vance G.},
    title = "{Persistent Staphylococcus aureus Bacteremia: Challenges and Controversies}",
    journal = {Clinical Infectious Diseases},
    volume = {75},
    number = {10},
    pages = {1863-1870},
    year = {2022},
    doi = {10.1093/cid/ciac4 persistent}
}

@article{Paulsen2024,
    author = {Paulsen, Johann and Giske, Christian G. and Frimodt-Møller, Niels and Knudsen, Jenny Dahl and Petersen, Andreas and Kjøbek, Lotte and Brandt, Carolin and Jensen, Uffe S. and Schønheyder, Henrik C. and Knudsen, Ida D. and Østergaard, Christian and Arpi, Magnus and Andersen, Claus and Tønder, Rikke V. and Søndergaard, Tove S. and Rosenvinge, Flemming S. and Møller, Jacob K. and Jensen, Thøger G. and Kjær, Jacob and Lindegaard, Bente and Benfield, Thomas},
    title = "{Ceftaroline vs Standard-of-Care Antibiotics for Treatment of Complicated Staphylococcus aureus Bacteremia: A Randomized Clinical Trial}",
    journal = {JAMA Internal Medicine},
    volume = {184},
    number = {2},
    pages = {143-151},
    year = {2024},
    doi = {10.1001/jamainternmed.2023.6764}
}

@article{Davis2023,
    author = {Davis, Joshua S. and Stevens, Vanessa and van Hal, Sebastiaan J.},
    title = "{Time to get personal with Staphylococcus aureus bacteraemia}",
    journal = {Clinical Microbiology and Infection},
    volume = {29},
    number = {11},
    pages = {1357-1359},
    year = {2023},
    doi = {https://doi.org/10.1016/j.cmi.2023.07.017}
}

@article{Siontis2014,
    author = {Siontis, George C.M. and Tzoulaki, Ioanna and Castaldi, Peter J. and Ioannidis, John P.A.},
    title = "{External validation of new risk prediction models is infrequent and reveals worse prognostic discrimination}",
    journal = {Journal of Clinical Epidemiology},
    volume = {68},
    number = {1},
    pages = {25-34},
    year = {2015},
    doi = {https://doi.org/10.1016/j.jclinepi.2014.09.007}
}

@article{Kent2018,
    author = {Kent, David M. and Steyerberg, Ewout W. and van Klaveren, David},
    title = "{Personalized evidence based medicine: predictive approaches to heterogeneous treatment effects}",
    journal = {BMJ},
    volume = {363},
    pages = {k4245},
    year = {2018},
    doi = {10.1136/bmj.k4245}
}

@article{Sussman2017,
    author = {Sussman, Jeremy B. and Hayward, Rodney A.},
    title = "{An IV for the Use of Subgroup Analysis in Randomized Trials}",
    journal = {Annals of Internal Medicine},
    volume = {153},
    number = {2},
    pages = {124-130},
    year = {2010},
    doi = {10.7326/0003-4819-153-2-201007200-00263}
}

@article{Anthenelli2011,
    author = {Anthenelli, Robert M. and Simon, Neal and O’Malley, Stephanie S. and Breslow, Roger and West, Robert and McRee, Bud and Hoffmann, David and Interpol, Claire and Meyer, Roger and Simon, Richard},
    title = "{An Evaluation of the Use of Biomarkers to Predict the Effects of Varenicline on Smoking Cessation and Safety}",
    journal = {Annals of Internal Medicine},
    volume = {155},
    number = {11},
    pages = {760-771},
    year = {2011},
    doi = {10.7326/0003-4819-155-11-201112060-00007}
}

@article{Simon2004,
    author = {Simon, Richard and Maitournam, Aboubakar},
    title = "{Evaluating the efficiency of targeted designs for randomized clinical trials}",
    journal = {Clinical Cancer Research},
    volume = {10},
    number = {19},
    pages = {6759-6763},
    year = {2004},
    doi = {10.1158/1078-0432.CCR-04-0721}
}

@article{Wang2007,
    author = {Wang, Rong and Lagakos, Stephen W. and Ware, James H. and Hunter, David J. and Drazen, Jeffrey M.},
    title = "{Statistics in Medicine — Reporting of Subgroup Analyses in Clinical Trials}",
    journal = {New England Journal of Medicine},
    volume = {357},
    number = {21},
    pages = {2189-2194},
    year = {2007},
    doi = {10.1056/NEJMsr077003}
}

@article{Pocock2007,
    author = {Pocock, Stuart J. and Assmann, Susan E. and Enos, Lori E. and Kasten, Linda E.},
    title = "{Subgroup analysis, covariate adjustment and baseline comparisons in clinical trial reporting: current practice and problems}",
    journal = {Statistics in Medicine},
    volume = {21},
    number = {19},
    pages = {2917-2930},
    year = {2002},
    doi = {https://doi.org/10.1002/sim.1296}
}

@article{Antoniou2016,
    author = {Antoniou, Michael and Jorgensen, Andrea L. and Kolamunnage-Dona, Ruwanthi},
    title = "{Biomarker-guided adaptive enrichment designs in clinical trials: A review of methods and challenges}",
    journal = {Contemporary Clinical Trials},
    volume = {48},
    pages = {104-114},
    year = {2016},
    doi = {https://doi.org/10.1016/j.cct.2016.04.006}
}

@article{Wang2014,
    author = {Wang, Sue-Jane and Hung, H. M. James and O'Neill, Robert T.},
    title = "{Adaptive enrichment trial design challenges and opportunities}",
    journal = {Biometrical Journal},
    volume = {56},
    number = {1},
    pages = {146-161},
    year = {2014},
    doi = {https://doi.org/10.1002/bimj.201300158}
}
```

# Supplementary Material


#| label: fig-supp-bias-arrest
#| fig-cap: "Bias Distribution vs Accuracy (ARREST ORs, N = 10,000). Bias calculated against target log(OR) for each group label."

# Plotting bias_val which compares estimated beta to true_beta_target
if (exists("results_aim2_processed") && nrow(results_aim2_processed) > 0) {
  # Filter out Overall group for this plot
  ggplot(results_aim2_processed %>% filter(group != "Overall"), aes(x = factor(accuracy), y = bias_val, fill = group)) +
    # Use boxplot to show distribution of bias
    geom_boxplot(position = position_dodge(width = 0.7), outlier.shape = NA) + # Hide outliers for clarity
    # Add horizontal line at zero bias for reference
    geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
    facet_wrap(~ group, scales = "free_y") + # Separate plot per subgroup
    labs(
      # title = paste("Bias Distribution vs Accuracy (ARREST; n =", n_fixed_aim2, ")"), # Redundant
      # subtitle = "Bias calculated against target log(OR) for each group label",
      x = "Classification Accuracy",
      y = "Bias (log OR)"
    ) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") # Hide redundant legend
} else {
  print("Data frame 'results_aim2_processed' not found or is empty. Cannot generate supplementary bias plot for ARREST ORs.")
}

#| label: fig-supp-bias-realistic
#| fig-cap: "Bias Distribution vs Accuracy (Realistic ORs, N = 10,000). Bias calculated against target log(OR) for each group label."

# Plotting bias_val for realistic scenario
if (exists("results_aim2_real_processed") && nrow(results_aim2_real_processed) > 0) {
  # Filter out Overall group
  ggplot(results_aim2_real_processed %>% filter(group != "Overall"), aes(x = factor(accuracy), y = bias_val, fill = group)) +
    geom_boxplot(position = position_dodge(width = 0.7), outlier.shape = NA) + # Hide outliers
    # Hline at zero bias
    geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
    facet_wrap(~ group, scales = "free_y") +
    labs(
      # title = paste("Bias Distribution vs Accuracy (Realistic; n =", n_fixed_aim2, ")"), # Redundant
      # subtitle = "Bias calculated against target log(OR) for each group label",
      x = "Classification Accuracy",
      y = "Bias (log OR)"
    ) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") # Hide redundant legend
} else {
  print("Data frame 'results_aim2_real_processed' not found or is empty. Cannot generate supplementary bias plot for Realistic ORs.")
}
```qmd


